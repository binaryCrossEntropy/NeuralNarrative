---
title: "Self-Attention Mechanism"
description: "Visual walkthrough of single-headed and multi-headed self-attention â€” understanding the matrix dimensions and operations."
date: 2024-01-15
tags: ["attention", "transformers", "deep-learning"]
category: "Attention"
---
import { Image } from 'astro:assets';
import single_head from '../../../assets/single_head.png';
import multi_head from '../../../assets/multi_head.png';

## Single Headed Attention

<Image src={single_head} alt="Single-headed attention mechanism diagram." />

- Ignore the Softmax operation and normalize by dividing by the square root of `d_model`, because these operations do not affect the dimensions of the matrices involved.

## Multi-Headed Attention

<Image src={multi_head} alt="Multi-headed attention mechanism diagram." />

- Ignore the Softmax operation and normalize by dividing by the square root of `d_model`, because these operations do not affect the dimensions of the matrices involved.
